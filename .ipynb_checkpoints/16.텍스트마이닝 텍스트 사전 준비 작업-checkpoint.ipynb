{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d165ab8c-3f10-49f4-b4cc-e7897332ee97",
   "metadata": {},
   "source": [
    "# 텍스트 사전 준비 작업 = 텍스트 정규화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea02fe7-7596-459c-821c-9b2867e212b2",
   "metadata": {},
   "source": [
    "## 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7c61f8-17de-4a71-b6ff-73113bc0a502",
   "metadata": {},
   "source": [
    "### 문장 토큰화\n",
    "- 문장의 마침표, 개행문자등 문장의 마지막을 뜻하는 기호에 따라 분리하는 것\n",
    "  nltk : https://www.nltk.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a19064b-98f4-46cf-9cf3-b9fea3c93963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\user\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6733eea4-6417-4cee-97b7-f5ae30c1fc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "text_sample = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
    "               You can see it out your window or on your television. \\\n",
    "               You feel it when you go to work, or go to church or pay your taxes.'\n",
    "sentence = sent_tokenize(text_sample)\n",
    "print(len(sentence))\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85869997-df01-4d8c-aa88-af92d019ccf5",
   "metadata": {},
   "source": [
    "### 단어 토큰화\n",
    "- 문장을 단어로 토큰화 하는 것\n",
    "- 일반적으로 문장 토큰화는 각 문장이 가지는 의미가 중요한 요소로 사용될때 사용\n",
    "- BoW(Bag of World)와 같이 단어의 순서가 중요하지 않는 경우 단어 토큰화만 해도 충분하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8604e6d4-c8eb-426e-94f5-0bc9efe6944a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "sentence = 'The Matrix is everywhere its all around us, here even in this room.'\n",
    "words = word_tokenize(sentence)\n",
    "print(len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e913f05-8a5a-4cc8-a7e5-cd7907cd9f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize, word_tokenize\n",
    "def tokenize_text(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens\n",
    "word_tokens = tokenize_text(text_sample)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e043c4f1-8347-4d63-b03a-2f75be686838",
   "metadata": {},
   "source": [
    "## stopwords 제거\n",
    "- 분석에 큰 의미가 없는 단어를 지칭한다\n",
    "- is, the, a, will 등 문맥적으로 큰 의미가 없는 단어가 이에 해당한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84f17809-f376-4555-a855-796863a1be98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords') #불용어 목록을 다운받는다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8688c444-16a7-4182-b056-25c63bdf3528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arabic', 'azerbaijani', 'basque', 'bengali', 'catalan', 'chinese', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'greek', 'hebrew', 'hinglish', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian', 'slovene', 'spanish', 'swedish', 'tajik', 'turkish']\n"
     ]
    }
   ],
   "source": [
    "print(nltk.corpus.stopwords.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9233f891-d523-4e82-b32f-2fd8dccf44bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 불용어 개수: 179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    }
   ],
   "source": [
    "print('영어 불용어 개수:',len(nltk.corpus.stopwords.words('english')) )\n",
    "print(nltk.corpus.stopwords.words('english')[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931f24d2-cd91-40ff-8ab7-5a7f54563ba3",
   "metadata": {},
   "source": [
    "# 구두점 목록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd008463-3823-4a5e-ba90-9e3e162ab81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6fb2a28c-7a53-4520-bb44-379ec3bc76ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[원본단어]\n",
      "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n",
      "[불용어 제거 단어]\n",
      "[['matrix', 'everywhere', 'around', 'us', 'even', 'room'], ['see', 'window', 'television'], ['feel', 'go', 'work', 'go', 'church', 'pay', 'taxes']]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "all_tokens = []\n",
    "print('[원본단어]')\n",
    "print(word_tokens)\n",
    "for sentence in word_tokens:\n",
    "    filtered_words = []\n",
    "    for word in sentence:\n",
    "        word = word.lower()\n",
    "        if word not in stopwords and word not in string.punctuation:\n",
    "            filtered_words.append(word)\n",
    "    all_tokens.append(filtered_words)\n",
    "print('[불용어 제거 단어]')\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7c0705-38ce-4cbd-835b-255a3c5daedc",
   "metadata": {},
   "source": [
    "## 어간 추출(Stemming)과 표제어 추출(Lemmatization)\n",
    "- 문법적 또는 의미적으로 변화하는 `단어의 원형을 찾는다.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "674eb86b-b9a1-466a-a9fb-3f0f52ecc639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "amus amus amus\n",
      "happy happiest\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "print(stemmer.stem('working'),stemmer.stem('works'), stemmer.stem('worked'))\n",
    "print(stemmer.stem('amusing'),stemmer.stem('amuses'), stemmer.stem('amused'))\n",
    "print(stemmer.stem('happier'),stemmer.stem('happiest'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "10142002-02c5-4cfd-9181-cdbd0673cd34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amuse amuse\n",
      "happy happy\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "print(lemma.lemmatize('amusing','v'), lemma.lemmatize('amused','v')) # v는 품사 : 동사 v 명사 n 형용사 a 부사 r\n",
    "print(lemma.lemmatize('happier','a'), lemma.lemmatize('happiest','a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4acde6d-04e0-4ede-9c3d-f6d6f309f524",
   "metadata": {},
   "source": [
    "# Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b76d2e-5317-497f-88fa-05f5a997152f",
   "metadata": {},
   "source": [
    "## DTM(Document Term Matrix, 문서 단어 행렬)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae59fa22-6898-439f-8ba9-98b314f59c53",
   "metadata": {},
   "source": [
    "### CounterVectorizer 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f33e7fad-6d31-497f-9f26-2b075ed12839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words vector: [[1 1 2 1 2 1]]\n",
      "vocabulary:  [('because', 0), ('know', 1), ('love', 2), ('want', 3), ('you', 4), ('your', 5)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = ['you know I want your love, because I love you.']\n",
    "vector = CountVectorizer()\n",
    "print('bag of words vector:',vector.fit_transform(corpus).toarray())\n",
    "# 'I'는 BoW를 만드는 과정에서 제외됨(CounterVecterizer는 기본적으로 길이가 2 이상인 단어만 토큰으로 인식)\n",
    "print('vocabulary: ', sorted(vector.vocabulary_.items(), key=lambda item:item[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2be4ffe-4a3e-4912-9750-9c94469e5d62",
   "metadata": {},
   "source": [
    "- 불용어를 제거한 BoW 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc147835-2adc-4b0e-986a-df02641510d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. 사용자 정의 불용어 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "257ba000-a236-4f49-8142-d5cdd7228d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1]]\n",
      "vocabulary: [('everything', 0), ('family', 1), ('important', 2), ('it', 3), ('thing', 4)]\n"
     ]
    }
   ],
   "source": [
    "text = [\"Family is not an important thing. It's everything\"]\n",
    "vector = CountVectorizer(stop_words=['the','an','a','is','not'])\n",
    "print(vector.fit_transform(text).toarray())\n",
    "print('vocabulary:', sorted(vector.vocabulary_.items(), key=lambda item:item[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b473e5e6-dcb3-4721-b4a9-03ff609786ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. CountVectorizer에서 제공하는 자체 불용어 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "42f8a422-8e41-4023-b9e7-d774003dc6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1]]\n",
      "vocabulary: [('family', 0), ('important', 1), ('thing', 2)]\n"
     ]
    }
   ],
   "source": [
    "text = [\"Family is not an important thing. It's everything.\"]\n",
    "vector = CountVectorizer(stop_words='english')\n",
    "print(vector.fit_transform(text).toarray())\n",
    "print('vocabulary:', sorted(vector.vocabulary_.items(), key=lambda item:item[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f60a562-2bd5-4f23-9346-269358c3c129",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. NLTK에서 지원하는 불용어 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "86996c5a-314e-42aa-9bdb-3a689c37e9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1]]\n",
      "vocabulary: [('everything', 0), ('family', 1), ('important', 2), ('thing', 3)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "text = [\"Family is not an important thing. It's everything.\"]\n",
    "stop_words = stopwords.words('english')\n",
    "vector = CountVectorizer(stop_words=stop_words)\n",
    "print(vector.fit_transform(text).toarray())\n",
    "print('vocabulary:', sorted(vector.vocabulary_.items(), key=lambda item:item[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eee39d7-6a93-4fb2-8140-94b318ef2f98",
   "metadata": {},
   "source": [
    "### TF-IDF(Term Frequency-Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "35d7d126-ecc2-48b7-8ab4-a55cf9acb525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46735098 0.         0.46735098 0.         0.46735098\n",
      "  0.         0.35543247 0.46735098]\n",
      " [0.         0.         0.79596054 0.         0.         0.\n",
      "  0.         0.60534851 0.        ]\n",
      " [0.57735027 0.         0.         0.         0.57735027 0.\n",
      "  0.57735027 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = ['you know I want your love', #문서 1\n",
    "         'I like you', #문서 2\n",
    "         'what should I do' #문서 3 \n",
    "         ]\n",
    "tfidf = TfidfVectorizer() \n",
    "print(tfidf.fit_transform(corpus).toarray())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
